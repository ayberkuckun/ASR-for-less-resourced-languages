{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset librispeech_asr (C:\\Users\\ayber\\.cache\\huggingface\\datasets\\hf-internal-testing___librispeech_asr\\clean\\2.1.0\\f2c70a4d03ab4410954901bde48c54b85ca1b7f9bf7d616e7e2a72b5ee6ddbfc)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "asd\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"hf-internal-testing/librispeech_asr_demo\", \"clean\", split=\"validation\")\n",
    "dataset\n",
    "print(\"asd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "'he tells us that at this festive season of the year with christmas and roast beef looming before us similes drawn from eating and its results occur most readily to the mind'"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_sample = dataset[2]\n",
    "audio_sample[\"text\"].lower()\n",
    "# print(\"asd\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/wav2vec2-base-100h were not used when initializing Wav2Vec2ForCTC: ['wav2vec2.mask_time_emb_vector']\n",
      "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-100h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n",
    "\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-100h\")\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-100h\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wav2Vec2ForCTC(\n",
      "  (wav2vec2): Wav2Vec2Model(\n",
      "    (feature_extractor): Wav2Vec2FeatureEncoder(\n",
      "      (conv_layers): ModuleList(\n",
      "        (0): Wav2Vec2GroupNormConvLayer(\n",
      "          (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)\n",
      "          (layer_norm): GroupNorm(512, 512, eps=1e-05, affine=True)\n",
      "        )\n",
      "        (1): Wav2Vec2NoLayerNormConvLayer(\n",
      "          (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
      "        )\n",
      "        (2): Wav2Vec2NoLayerNormConvLayer(\n",
      "          (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
      "        )\n",
      "        (3): Wav2Vec2NoLayerNormConvLayer(\n",
      "          (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
      "        )\n",
      "        (4): Wav2Vec2NoLayerNormConvLayer(\n",
      "          (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
      "        )\n",
      "        (5): Wav2Vec2NoLayerNormConvLayer(\n",
      "          (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)\n",
      "        )\n",
      "        (6): Wav2Vec2NoLayerNormConvLayer(\n",
      "          (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (feature_projection): Wav2Vec2FeatureProjection(\n",
      "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (projection): Linear(in_features=512, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): Wav2Vec2Encoder(\n",
      "      (pos_conv_embed): Wav2Vec2PositionalConvEmbedding(\n",
      "        (conv): Conv1d(768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)\n",
      "        (padding): Wav2Vec2SamePadLayer()\n",
      "      )\n",
      "      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (layers): ModuleList(\n",
      "        (0): Wav2Vec2EncoderLayer(\n",
      "          (attention): Wav2Vec2Attention(\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (feed_forward): Wav2Vec2FeedForward(\n",
      "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (1): Wav2Vec2EncoderLayer(\n",
      "          (attention): Wav2Vec2Attention(\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (feed_forward): Wav2Vec2FeedForward(\n",
      "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (2): Wav2Vec2EncoderLayer(\n",
      "          (attention): Wav2Vec2Attention(\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (feed_forward): Wav2Vec2FeedForward(\n",
      "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (3): Wav2Vec2EncoderLayer(\n",
      "          (attention): Wav2Vec2Attention(\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (feed_forward): Wav2Vec2FeedForward(\n",
      "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (4): Wav2Vec2EncoderLayer(\n",
      "          (attention): Wav2Vec2Attention(\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (feed_forward): Wav2Vec2FeedForward(\n",
      "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (5): Wav2Vec2EncoderLayer(\n",
      "          (attention): Wav2Vec2Attention(\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (feed_forward): Wav2Vec2FeedForward(\n",
      "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (6): Wav2Vec2EncoderLayer(\n",
      "          (attention): Wav2Vec2Attention(\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (feed_forward): Wav2Vec2FeedForward(\n",
      "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (7): Wav2Vec2EncoderLayer(\n",
      "          (attention): Wav2Vec2Attention(\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (feed_forward): Wav2Vec2FeedForward(\n",
      "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (8): Wav2Vec2EncoderLayer(\n",
      "          (attention): Wav2Vec2Attention(\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (feed_forward): Wav2Vec2FeedForward(\n",
      "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (9): Wav2Vec2EncoderLayer(\n",
      "          (attention): Wav2Vec2Attention(\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (feed_forward): Wav2Vec2FeedForward(\n",
      "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (10): Wav2Vec2EncoderLayer(\n",
      "          (attention): Wav2Vec2Attention(\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (feed_forward): Wav2Vec2FeedForward(\n",
      "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (11): Wav2Vec2EncoderLayer(\n",
      "          (attention): Wav2Vec2Attention(\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (feed_forward): Wav2Vec2FeedForward(\n",
      "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (lm_head): Linear(in_features=768, out_features=32, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "asd\n"
     ]
    }
   ],
   "source": [
    "inputs = processor(audio_sample[\"audio\"][\"array\"], sampling_rate=audio_sample[\"audio\"][\"sampling_rate\"], return_tensors=\"pt\")\n",
    "print(\"asd\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "asd\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "with torch.no_grad():\n",
    "  logits = model(**inputs).logits\n",
    "\n",
    "print(\"asd\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "asd\n"
     ]
    }
   ],
   "source": [
    "predicted_ids = torch.argmax(logits, dim=-1)\n",
    "transcription = processor.batch_decode(predicted_ids)\n",
    "\n",
    "transcription[0].lower()\n",
    "\n",
    "print(\"asd\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2ProcessorWithLM\n",
    "\n",
    "processor = Wav2Vec2ProcessorWithLM.from_pretrained(\"patrickvonplaten/wav2vec2-base-100h-with-lm\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1, 624, 32])"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "\"' </s> <pad> <s> <unk> A B C D E F G H I J K L M N O P Q R S T U V W X Y Z |\""
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(sorted(processor.tokenizer.get_vocab()))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot find context for 'fork'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Input \u001B[1;32mIn [14]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[1;34m()\u001B[0m\n\u001B[1;32m----> 1\u001B[0m transcription \u001B[38;5;241m=\u001B[39m \u001B[43mprocessor\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbatch_decode\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlogits\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnumpy\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mtext\n",
      "File \u001B[1;32mc:\\users\\ayber\\pycharmprojects\\asr-for-less-resourced-languages\\venv\\lib\\site-packages\\transformers\\models\\wav2vec2_with_lm\\processing_wav2vec2_with_lm.py:303\u001B[0m, in \u001B[0;36mWav2Vec2ProcessorWithLM.batch_decode\u001B[1;34m(self, logits, num_processes, beam_width, beam_prune_logp, token_min_logp, hotwords, hotword_weight)\u001B[0m\n\u001B[0;32m    301\u001B[0m \u001B[38;5;66;03m# create multiprocessing pool and list numpy arrays\u001B[39;00m\n\u001B[0;32m    302\u001B[0m logits_list \u001B[38;5;241m=\u001B[39m [array \u001B[38;5;28;01mfor\u001B[39;00m array \u001B[38;5;129;01min\u001B[39;00m logits]\n\u001B[1;32m--> 303\u001B[0m pool \u001B[38;5;241m=\u001B[39m \u001B[43mget_context\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mfork\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mPool(num_processes)\n\u001B[0;32m    305\u001B[0m \u001B[38;5;66;03m# pyctcdecode\u001B[39;00m\n\u001B[0;32m    306\u001B[0m decoded_beams \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdecoder\u001B[38;5;241m.\u001B[39mdecode_beams_batch(\n\u001B[0;32m    307\u001B[0m     pool,\n\u001B[0;32m    308\u001B[0m     logits_list\u001B[38;5;241m=\u001B[39mlogits_list,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    313\u001B[0m     hotword_weight\u001B[38;5;241m=\u001B[39mhotword_weight,\n\u001B[0;32m    314\u001B[0m )\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\multiprocessing\\context.py:239\u001B[0m, in \u001B[0;36mDefaultContext.get_context\u001B[1;34m(self, method)\u001B[0m\n\u001B[0;32m    237\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_actual_context\n\u001B[0;32m    238\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 239\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_context\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmethod\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\multiprocessing\\context.py:193\u001B[0m, in \u001B[0;36mBaseContext.get_context\u001B[1;34m(self, method)\u001B[0m\n\u001B[0;32m    191\u001B[0m     ctx \u001B[38;5;241m=\u001B[39m _concrete_contexts[method]\n\u001B[0;32m    192\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m:\n\u001B[1;32m--> 193\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcannot find context for \u001B[39m\u001B[38;5;132;01m%r\u001B[39;00m\u001B[38;5;124m'\u001B[39m \u001B[38;5;241m%\u001B[39m method) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[0;32m    194\u001B[0m ctx\u001B[38;5;241m.\u001B[39m_check_available()\n\u001B[0;32m    195\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m ctx\n",
      "\u001B[1;31mValueError\u001B[0m: cannot find context for 'fork'"
     ]
    }
   ],
   "source": [
    "transcription = processor.batch_decode(logits.numpy()).text"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot find context for 'fork'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Input \u001B[1;32mIn [13]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[1;34m()\u001B[0m\n\u001B[1;32m----> 1\u001B[0m transcription \u001B[38;5;241m=\u001B[39m \u001B[43mprocessor\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbatch_decode\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlogits\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnumpy\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mtext\n",
      "File \u001B[1;32mc:\\users\\ayber\\pycharmprojects\\asr-for-less-resourced-languages\\venv\\lib\\site-packages\\transformers\\models\\wav2vec2_with_lm\\processing_wav2vec2_with_lm.py:303\u001B[0m, in \u001B[0;36mWav2Vec2ProcessorWithLM.batch_decode\u001B[1;34m(self, logits, num_processes, beam_width, beam_prune_logp, token_min_logp, hotwords, hotword_weight)\u001B[0m\n\u001B[0;32m    301\u001B[0m \u001B[38;5;66;03m# create multiprocessing pool and list numpy arrays\u001B[39;00m\n\u001B[0;32m    302\u001B[0m logits_list \u001B[38;5;241m=\u001B[39m [array \u001B[38;5;28;01mfor\u001B[39;00m array \u001B[38;5;129;01min\u001B[39;00m logits]\n\u001B[1;32m--> 303\u001B[0m pool \u001B[38;5;241m=\u001B[39m \u001B[43mget_context\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mfork\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mPool(num_processes)\n\u001B[0;32m    305\u001B[0m \u001B[38;5;66;03m# pyctcdecode\u001B[39;00m\n\u001B[0;32m    306\u001B[0m decoded_beams \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdecoder\u001B[38;5;241m.\u001B[39mdecode_beams_batch(\n\u001B[0;32m    307\u001B[0m     pool,\n\u001B[0;32m    308\u001B[0m     logits_list\u001B[38;5;241m=\u001B[39mlogits_list,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    313\u001B[0m     hotword_weight\u001B[38;5;241m=\u001B[39mhotword_weight,\n\u001B[0;32m    314\u001B[0m )\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\multiprocessing\\context.py:239\u001B[0m, in \u001B[0;36mDefaultContext.get_context\u001B[1;34m(self, method)\u001B[0m\n\u001B[0;32m    237\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_actual_context\n\u001B[0;32m    238\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 239\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_context\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmethod\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\multiprocessing\\context.py:193\u001B[0m, in \u001B[0;36mBaseContext.get_context\u001B[1;34m(self, method)\u001B[0m\n\u001B[0;32m    191\u001B[0m     ctx \u001B[38;5;241m=\u001B[39m _concrete_contexts[method]\n\u001B[0;32m    192\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m:\n\u001B[1;32m--> 193\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcannot find context for \u001B[39m\u001B[38;5;132;01m%r\u001B[39;00m\u001B[38;5;124m'\u001B[39m \u001B[38;5;241m%\u001B[39m method) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[0;32m    194\u001B[0m ctx\u001B[38;5;241m.\u001B[39m_check_available()\n\u001B[0;32m    195\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m ctx\n",
      "\u001B[1;31mValueError\u001B[0m: cannot find context for 'fork'"
     ]
    }
   ],
   "source": [
    "transcription = processor.batch_decode(logits.numpy()).text\n",
    "transcription[0].lower()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'charmap' codec can't decode byte 0x8d in position 122544: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mUnicodeDecodeError\u001B[0m                        Traceback (most recent call last)",
      "Input \u001B[1;32mIn [3]\u001B[0m, in \u001B[0;36m<cell line: 3>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtransformers\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m AutoProcessor\n\u001B[1;32m----> 3\u001B[0m processor \u001B[38;5;241m=\u001B[39m \u001B[43mAutoProcessor\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mhf-test/xls-r-300m-sv\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mc:\\users\\ayber\\pycharmprojects\\asr-for-less-resourced-languages\\venv\\lib\\site-packages\\transformers\\models\\auto\\processing_auto.py:161\u001B[0m, in \u001B[0;36mAutoProcessor.from_pretrained\u001B[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001B[0m\n\u001B[0;32m    159\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mprocessor_class\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m config_dict:\n\u001B[0;32m    160\u001B[0m         processor_class \u001B[38;5;241m=\u001B[39m processor_class_from_name(config_dict[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mprocessor_class\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[1;32m--> 161\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m processor_class\u001B[38;5;241m.\u001B[39mfrom_pretrained(pretrained_model_name_or_path, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    163\u001B[0m \u001B[38;5;66;03m# Next, let's check whether the processor class is saved in a tokenizer\u001B[39;00m\n\u001B[0;32m    164\u001B[0m \u001B[38;5;66;03m# Let's start by checking whether the processor class is saved in a feature extractor\u001B[39;00m\n\u001B[0;32m    165\u001B[0m tokenizer_config_file \u001B[38;5;241m=\u001B[39m get_file_from_repo(\n\u001B[0;32m    166\u001B[0m     pretrained_model_name_or_path, TOKENIZER_CONFIG_FILE, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mget_file_from_repo_kwargs\n\u001B[0;32m    167\u001B[0m )\n",
      "File \u001B[1;32mc:\\users\\ayber\\pycharmprojects\\asr-for-less-resourced-languages\\venv\\lib\\site-packages\\transformers\\models\\wav2vec2_with_lm\\processing_wav2vec2_with_lm.py:174\u001B[0m, in \u001B[0;36mWav2Vec2ProcessorWithLM.from_pretrained\u001B[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001B[0m\n\u001B[0;32m    171\u001B[0m     alphabet_filename \u001B[38;5;241m=\u001B[39m BeamSearchDecoderCTC\u001B[38;5;241m.\u001B[39m_ALPHABET_SERIALIZED_FILENAME\n\u001B[0;32m    172\u001B[0m     allow_regex \u001B[38;5;241m=\u001B[39m [language_model_filenames, alphabet_filename]\n\u001B[1;32m--> 174\u001B[0m     decoder \u001B[38;5;241m=\u001B[39m BeamSearchDecoderCTC\u001B[38;5;241m.\u001B[39mload_from_hf_hub(\n\u001B[0;32m    175\u001B[0m         pretrained_model_name_or_path, allow_regex\u001B[38;5;241m=\u001B[39mallow_regex, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[0;32m    176\u001B[0m     )\n\u001B[0;32m    178\u001B[0m \u001B[38;5;66;03m# set language model attributes\u001B[39;00m\n\u001B[0;32m    179\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m attribute \u001B[38;5;129;01min\u001B[39;00m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124malpha\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbeta\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124munk_score_offset\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mscore_boundary\u001B[39m\u001B[38;5;124m\"\u001B[39m]:\n",
      "File \u001B[1;32mc:\\users\\ayber\\pycharmprojects\\asr-for-less-resourced-languages\\venv\\lib\\site-packages\\pyctcdecode\\decoder.py:759\u001B[0m, in \u001B[0;36mBeamSearchDecoderCTC.load_from_hf_hub\u001B[1;34m(cls, model_id, cache_dir, **kwargs)\u001B[0m\n\u001B[0;32m    752\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m(\n\u001B[0;32m    753\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYou need to install huggingface_hub to use `load_from_hf_hub`. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    754\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSee https://pypi.org/project/huggingface-hub/ for installation.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    755\u001B[0m     )\n\u001B[0;32m    757\u001B[0m cached_directory \u001B[38;5;241m=\u001B[39m snapshot_download(model_id, cache_dir\u001B[38;5;241m=\u001B[39mcache_dir, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m--> 759\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mcls\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload_from_dir\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcached_directory\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mc:\\users\\ayber\\pycharmprojects\\asr-for-less-resourced-languages\\venv\\lib\\site-packages\\pyctcdecode\\decoder.py:726\u001B[0m, in \u001B[0;36mBeamSearchDecoderCTC.load_from_dir\u001B[1;34m(cls, filepath)\u001B[0m\n\u001B[0;32m    724\u001B[0m     language_model \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    725\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 726\u001B[0m     language_model \u001B[38;5;241m=\u001B[39m \u001B[43mLanguageModel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload_from_dir\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilenames\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mlanguage_model\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    727\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mcls\u001B[39m(alphabet, language_model\u001B[38;5;241m=\u001B[39mlanguage_model)\n",
      "File \u001B[1;32mc:\\users\\ayber\\pycharmprojects\\asr-for-less-resourced-languages\\venv\\lib\\site-packages\\pyctcdecode\\language_model.py:377\u001B[0m, in \u001B[0;36mLanguageModel.load_from_dir\u001B[1;34m(cls, filepath)\u001B[0m\n\u001B[0;32m    371\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    372\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mExpected json serialized attributes to be \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39mJSON_ATTRS\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    373\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbut found \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mjson_attrs\u001B[38;5;241m.\u001B[39mkeys()\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    374\u001B[0m     )\n\u001B[0;32m    376\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(filenames[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124munigrams\u001B[39m\u001B[38;5;124m\"\u001B[39m], \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m fi:\n\u001B[1;32m--> 377\u001B[0m     unigrams \u001B[38;5;241m=\u001B[39m \u001B[43mfi\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39msplitlines()\n\u001B[0;32m    379\u001B[0m kenlm_model \u001B[38;5;241m=\u001B[39m kenlm\u001B[38;5;241m.\u001B[39mModel(filenames[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mkenlm\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[0;32m    380\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mcls\u001B[39m(kenlm_model, unigrams, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mjson_attrs)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\encodings\\cp1252.py:23\u001B[0m, in \u001B[0;36mIncrementalDecoder.decode\u001B[1;34m(self, input, final)\u001B[0m\n\u001B[0;32m     22\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecode\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m, final\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m):\n\u001B[1;32m---> 23\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mcodecs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcharmap_decode\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43merrors\u001B[49m\u001B[43m,\u001B[49m\u001B[43mdecoding_table\u001B[49m\u001B[43m)\u001B[49m[\u001B[38;5;241m0\u001B[39m]\n",
      "\u001B[1;31mUnicodeDecodeError\u001B[0m: 'charmap' codec can't decode byte 0x8d in position 122544: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "from transformers import AutoProcessor\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"hf-test/xls-r-300m-sv\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'processor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[1;32mIn [2]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[1;34m()\u001B[0m\n\u001B[1;32m----> 1\u001B[0m vocab_dict \u001B[38;5;241m=\u001B[39m \u001B[43mprocessor\u001B[49m\u001B[38;5;241m.\u001B[39mtokenizer\u001B[38;5;241m.\u001B[39mget_vocab()\n\u001B[0;32m      2\u001B[0m sorted_vocab_dict \u001B[38;5;241m=\u001B[39m {k\u001B[38;5;241m.\u001B[39mlower(): v \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28msorted\u001B[39m(vocab_dict\u001B[38;5;241m.\u001B[39mitems(), key\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mlambda\u001B[39;00m item: item[\u001B[38;5;241m1\u001B[39m])}\n",
      "\u001B[1;31mNameError\u001B[0m: name 'processor' is not defined"
     ]
    }
   ],
   "source": [
    "vocab_dict = processor.tokenizer.get_vocab()\n",
    "sorted_vocab_dict = {k.lower(): v for k, v in sorted(vocab_dict.items(), key=lambda item: item[1])}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}